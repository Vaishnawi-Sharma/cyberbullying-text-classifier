## 📊 Results 

This folder contains the *final outputs* of my Machine Learning models trained for toxic comment detection.

---

### 📝 logistic_predictions.csv
This file stores the evaluation results of my *Logistic Regression model* on the test dataset.  
Each row corresponds to a comment from the dataset.

*Columns:*
- *text* → the original comment text  
- *actual* → the ground truth label (0 = non-toxic, 1 = toxic)  
- *predicted* → the model’s prediction (0 = non-toxic, 1 = toxic)  

---

### 📊 Bar Chart Comparison of Results
The bar chart comparison is based on *Precision, Recall, and F1-score* between *Logistic Regression Model* and *SVM Model*.  

---

### 🔲 Confusion Matrix Comparison
The Confusion Matrix comparison between *Logistic Regression* and *SVM* based on *F1-score*, which showcases:  

- *True Positive (TP):* The true toxic comments predicted by our model (cell *[1][1]*).  
- *True Negative (TN):* The true non-toxic comments predicted by our model (cell *[0][0]*).  
- *False Negative (FN):* A toxic comment that the model wrongly predicted as safe (cell *[1][0]*).  
- *False Positive (FP):* A safe (non-toxic) comment that the model wrongly predicted as toxic (cell *[0][1]*).
